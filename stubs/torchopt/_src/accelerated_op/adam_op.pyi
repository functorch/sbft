"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional, Tuple

class AdamOp:
    """Fused accelerated Adam operators."""
    class MuOp(torch.autograd.Function):
        """Bias-corrected first moment estimate."""
        @staticmethod
        def jvp(ctx: Any, *grad_inputs: Any) -> Any:
            """Defines a formula for differentiating the operation with forward mode automatic differentiation."""
            ...
        
        @staticmethod
        def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:
            """Performs the operation."""
            ...
        
        @staticmethod
        def backward(ctx: Any, *args: Any) -> Any:
            """Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the :meth:`vjp` method)."""
            ...
        
    
    
    class NuOp(torch.autograd.Function):
        """Bias-corrected second raw moment estimate."""
        @staticmethod
        def jvp(ctx: Any, *grad_inputs: Any) -> Any:
            """Defines a formula for differentiating the operation with forward mode automatic differentiation."""
            ...
        
        @staticmethod
        def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:
            """Performs the operation."""
            ...
        
        @staticmethod
        def backward(ctx: Any, *args: Any) -> Any:
            """Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the :meth:`vjp` function)."""
            ...
        
    
    
    class UpdatesOp(torch.autograd.Function):
        """Adam updates."""
        @staticmethod
        def jvp(ctx: Any, *grad_inputs: Any) -> Any:
            """Defines a formula for differentiating the operation with forward mode automatic differentiation."""
            ...
        
        @staticmethod
        def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:
            """Performs the operation."""
            ...
        
        @staticmethod
        def backward(ctx: Any, *args: Any) -> Any:
            """Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the :meth:`vjp` function)."""
            ...
        
    
    
    def __init__(self, b1: float = ..., b2: float = ..., eps: float = ..., *, eps_root: float = ..., inplace: bool = ...) -> None:
        """The :meth:`__init__` function."""
        ...
    
    def __call__(self, mu: torch.Tensor, nu: torch.Tensor, updates: Optional[torch.Tensor], count: int) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """The :meth:`__call__` function."""
        ...
    


