"""
This type stub file was generated by pyright.
"""

import torch
from typing import Callable, Optional, Union
from torchopt._src.typing import TensorTree

def tree_inner_product(tree_x: TensorTree, tree_y: TensorTree) -> float:
    """Computes (tree_x.conj() * tree_y).real.sum()."""
    ...

def cg(A: Union[torch.Tensor, Callable[[TensorTree], TensorTree]], b: TensorTree, x0: Optional[TensorTree] = ..., *, rtol: float = ..., atol: float = ..., maxiter: Optional[int] = ..., M: Optional[Union[torch.Tensor, Callable[[TensorTree], TensorTree]]] = ...) -> TensorTree:
    """Use Conjugate Gradient iteration to solve ``Ax = b``.

    The numerics of JAX's ``cg`` should exact match SciPy's ``cg`` (up to numerical precision), but
    note that the interface is slightly different: you need to supply the linear operator ``A`` as a
    function instead of a sparse matrix or ``LinearOperator``.

    Derivatives of :func:`cg` are implemented via implicit differentiation with another :func:`cg`
    solve, rather than by differentiating *through* the solver. They will be accurate only if both
    solves converge.

    Args:
        A: (tensor or tree of tensors or function)
            2D array or function that calculates the linear map (matrix-vector
            product) ``Ax`` when called like ``A(x)``. ``A`` must represent a
            hermitian, positive definite matrix, and must return array(s) with the
            same structure and shape as its argument.
        b: (tensor or tree of tensors)
            Right hand side of the linear system representing a single vector. Can be
            stored as an array or Python container of array(s) with any shape.
        x0: (tensor or tree of tensors, optional)
            Starting guess for the solution. Must have the same structure as ``b``.
        rtol: (float, optional, default: :const:`1e-5`)
            Tolerances for convergence, ``norm(residual) <= max(rtol*norm(b), atol)``. We do not
            implement SciPy's "legacy" behavior, so JAX's tolerance will differ from SciPy unless
            you explicitly pass ``atol`` to SciPy's ``cg``.
        atol: (float, optional, default: :const:`0.0`)
            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``. We do not
            implement SciPy's "legacy" behavior, so JAX's tolerance will differ from SciPy unless
            you explicitly pass ``atol`` to SciPy's ``cg``.
        maxiter: (integer, optional)
            Maximum number of iterations. Iteration will stop after maxiter steps even if the
            specified tolerance has not been achieved.
        M: (tensor or tree of tensors or function)
            Pre-conditioner for ``A``. The pre-conditioner should approximate the inverse of ``A``.
            Effective preconditioning dramatically improves the rate of convergence, which implies
            that fewer iterations are needed to reach a given error tolerance.

    Returns:
        the Conjugate Gradient (CG) linear solver
    """
    ...

