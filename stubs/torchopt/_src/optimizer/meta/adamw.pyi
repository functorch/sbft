"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from typing import Any, Callable, Optional, Tuple, Union
from torchopt._src import base
from torchopt._src.optimizer.meta.base import MetaOptimizer
from torchopt._src.typing import ScalarOrSchedule

class MetaAdamW(MetaOptimizer):
    """The differentiable AdamW optimizer.

    See Also:
        - The functional AdamW optimizer: :func:`torchopt.adamw`.
        - The classic AdamW optimizer: :class:`torchopt.AdamW`.
    """
    def __init__(self, net: nn.Module, lr: ScalarOrSchedule = ..., betas: Tuple[float, float] = ..., eps: float = ..., weight_decay: float = ..., *, eps_root: float = ..., mask: Optional[Union[Any, Callable[[base.Params], Any]]] = ..., moment_requires_grad: bool = ..., maximize: bool = ..., use_accelerated_op: bool = ...) -> None:
        """The :meth:`init` function.

        Args:
            net: (nn.Module)
                A network whose parameters should be optimized.
            lr: (default: :const:`1e-3`)
                This is a fixed global scaling factor.
            betas: (default: :const:`(0.9, 0.999)`)
                Coefficients used for computing running averages of gradient and its square.
            eps: (default: :const:`1e-8`)
                A small constant applied to denominator outside of the square root (as in the Adam
                paper) to avoid dividing by zero when rescaling.
            weight_decay: (default: :const:`1e-2`)
                Strength of the weight decay regularization. Note that this weight decay is
                multiplied with the learning rate. This is consistent with other frameworks such as
                PyTorch, but different from (Loshchilov et al, 2019) where the weight decay is only
                multiplied with the "schedule multiplier", but not the base learning rate.
            eps_root: (default: :data:`0.0`)
                A small constant applied to denominator inside the square root (as in RMSProp), to
                avoid dividing by zero when rescaling. This is needed for example when computing
                (meta-)gradients through Adam.
            mask: (default: :data:`None`)
                A tree with same structure as (or a prefix of) the params PyTree, or a Callable that
                returns such a pytree given the params/updates. The leaves should be booleans,
                :data:`True` for leaves/subtrees you want to apply the weight decay to, and
                :data:`False` for those you want to skip. Note that the Adam gradient
                transformations are applied to all parameters.
            moment_requires_grad: (default: :data:`False`)
                If :data:`True` the momentums will be created with flag ``requires_grad=True``, this
                flag is often used in Meta-Learning algorithms.
            maximize: (default: :data:`False`)
                Maximize the params based on the objective, instead of minimizing.
            use_accelerated_op: (default: :data:`False`)
                If :data:`True` use our implemented fused operator.
        """
        ...
    


