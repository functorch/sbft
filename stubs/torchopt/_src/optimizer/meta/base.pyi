"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from torchopt._src.base import GradientTransformation

class MetaOptimizer:
    """The base class for high-level differentiable optimizers."""
    def __init__(self, net: nn.Module, impl: GradientTransformation) -> None:
        """The :meth:`init` function.

        Args:
            net: (nn.Module)
                A network whose parameters should be optimized.
            impl: (GradientTransformation)
                A low level optimizer function, it could be a optimizer function provided by
                ``alias.py`` or a customized ``chain`` provided by ``combine.py``.
                Note that using ``MetaOptimizer(sgd(moment_requires_grad=True))`` or
                ``MetaOptimizer(chain(sgd(moment_requires_grad=True)))`` is equivalent to
                :class:`torchopt.MetaSGD`.
        """
        ...
    
    def step(self, loss: torch.Tensor): # -> None:
        """Compute the gradients of the loss to the network parameters and update network parameters.

        Graph of the derivative will be constructed, allowing to compute higher order derivative
        products. We use the differentiable optimizer (pass argument ``inplace=False``) to scale the
        gradients and update the network parameters without modifying tensors in-place.

        Args:
            loss: (torch.Tensor)
                The loss that is used to compute the gradients to the network parameters.
        """
        ...
    
    def add_param_group(self, net): # -> None:
        """Add a param group to the optimizer's :attr:`state_groups`."""
        ...
    
    def state_dict(self): # -> tuple[Unknown, ...]:
        """Extract the references of the optimizer states.

        Note that the states are references, so any in-place operations will change the states
        inside :class:`MetaOptimizer` at the same time.
        """
        ...
    
    def load_state_dict(self, state_dict): # -> None:
        """Load the references of the optimizer states."""
        ...
    


