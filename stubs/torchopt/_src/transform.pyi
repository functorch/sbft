"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Callable, List, NamedTuple, Optional, Sequence, Union
from torchopt._src import base
from torchopt._src.typing import Schedule
from torchopt._src.utils import pytree

ScaleState = base.EmptyState
INT32_MAX = torch.iinfo(torch.int32).max
TRIPLE_PYTREEDEF = pytree.tree_structure((0, 1, 2))
def map_flattened(func: Callable, *args: Any) -> List[Any]:
    """Apply a function to each element of a flattened list."""
    ...

def with_flattened_tree(inner: base.GradientTransformation) -> base.GradientTransformation:
    """Wraps around the inner transformation that manipulates the flattened tree structure (:class:``list``)."""
    ...

def inc_count(updates: base.Updates, count: Sequence[torch.Tensor]) -> Sequence[torch.Tensor]:
    """Increments int counter by one.

    Returns:
        A counter incremeted by one, or max_int if the maximum precision is reached.
    """
    ...

def scale(step_size: float) -> base.GradientTransformation:
    """Scale updates by some fixed scalar ``step_size``.

    Args:
        step_size: A scalar corresponding to a fixed scaling factor for updates.

    Returns:
        An ``(init_fn, update_fn)`` tuple.
    """
    ...

class ScaleByScheduleState(NamedTuple):
    """Maintains count for scale scheduling."""
    count: Sequence[torch.Tensor]
    ...


def scale_by_schedule(step_size_fn: Schedule) -> base.GradientTransformation:
    """Scale updates using a custom schedule for the ``step_size``.

    Args:
        step_size_fn:
            A function that takes an update count as input and proposes the ``step_size`` to
            multiply the updates by.

    Returns:
        An ``(init_fn, update_fn)`` tuple.
    """
    ...

class ScaleByAdamState(NamedTuple):
    """State for the Adam algorithm."""
    mu: base.Updates
    nu: base.Updates
    count: Sequence[torch.Tensor]
    ...


def scale_by_adam(b1: float = ..., b2: float = ..., eps: float = ..., eps_root: float = ..., moment_requires_grad: bool = ...) -> base.GradientTransformation:
    """Rescale updates according to the Adam algorithm.

    References:
        [Kingma et al, 2014](https://arxiv.org/abs/1412.6980)

    Args:
        b1: (default: :const:`0.9`)
            Decay rate for the exponentially weighted average of grads.
        b2: (default: :const:`0.999`)
            Decay rate for the exponentially weighted average of squared grads.
        eps: (default: :const:`1e-8`)
            Term added to the denominator to improve numerical stability.
        eps_root: (default: :const:`0.0`)
            Term added to the denominator inside the square-root to improve
            numerical stability when back-propagating gradients through the rescaling.
        moment_requires_grad: (default: :data:`False`)
            if :data:`True`, states will be created with flag `requires_grad = True`.

    Returns:
        An (init_fn, update_fn) tuple.
    """
    ...

def scale_by_accelerated_adam(b1: float = ..., b2: float = ..., eps: float = ..., eps_root: float = ..., moment_requires_grad: bool = ...) -> base.GradientTransformation:
    """Rescale updates according to the Adam algorithm.

    This function is accelerated by using some fused accelerated operators.

    References:
        [Kingma et al, 2014](https://arxiv.org/abs/1412.6980)

    Args:
        b1: (default: :const:`0.9`)
            Decay rate for the exponentially weighted average of grads.
        b2: (default: :const:`0.999`)
            Decay rate for the exponentially weighted average of squared grads.
        eps: (default: :const:`1e-8`)
            Term added to the denominator to improve numerical stability.
        eps_root: (default: :const:`0.0`)
            Term added to the denominator inside the square-root to improve
            numerical stability when back-propagating gradients through the rescaling.
        moment_requires_grad: (default: :data:`False`)
            if :data:`True`, states will be created with flag `requires_grad = True`.

    Returns:
        An (init_fn, update_fn) tuple.
    """
    ...

class TraceState(NamedTuple):
    """Holds an aggregation of past updates."""
    trace: base.Params
    ...


def trace(momentum: float = ..., dampening: float = ..., nesterov: bool = ..., moment_requires_grad: bool = ...) -> base.GradientTransformation:
    """Compute a trace of past updates.

    Note: `trace` and `ema` have very similar but distinct updates;
    `trace = decay * trace + t`, while `ema = decay * ema + (1 - decay) * t`.
    Both are frequently found in the optimization literature.

    Args:
        momentum: (default: :const:`0.9`)
            The decay rate for the trace of past updates.
        dampening: (default: :const:`0.0`)
            Dampening for momentum.
        nesterov: (default: :data:`False`)
            Whether to use Nesterov momentum.
        moment_requires_grad: (default: :data:`False`)
            if :data:`True`, states will be created with flag `requires_grad = True`.

    Returns:
        An (init_fn, update_fn) tuple.
    """
    ...

class ScaleByRmsState(NamedTuple):
    """State for exponential root mean-squared (RMS)-normalized updates."""
    nu: base.Updates
    ...


def scale_by_rms(alpha: float = ..., eps: float = ..., initial_scale: float = ...) -> base.GradientTransformation:
    """Rescale updates by the root of the exp. moving avg of the square.

    References:
        [Hinton](www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)

    Args:
        alpha: (default: :const:`0.9`)
            Decay rate for the exponentially weighted average of squared grads.
        eps: (default: :const:`1e-8`)
            Term added to the denominator to improve numerical stability.
        initial_scale: (default: :const:`0.0`)
            Initial value for second moment

    Returns:
        An (init_fn, update_fn) tuple.
    """
    ...

class ScaleByRStdDevState(NamedTuple):
    """State for centered exponential moving average of squares of updates."""
    mu: base.Updates
    nu: base.Updates
    ...


def scale_by_stddev(alpha: float = ..., eps: float = ..., initial_scale: float = ...) -> base.GradientTransformation:
    """Rescale updates by the root of the centered exp. moving average of squares.

    References:
        [Hinton](www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)

    Args:
        alpha: (default: :const:`0.9`)
            Decay rate for the exponentially weighted average of squared grads.
        eps: (default: :const:`1e-8`)
            Term added to the denominator to improve numerical stability.
        initial_scale: (default: :const:`0.0`)
            Initial value for second moment

    Returns:
        An (init_fn, update_fn) tuple.
    """
    ...

class MaskedState(NamedTuple):
    """Maintains inner transform state for masked transformations."""
    inner_state: Any
    ...


class MaskedNode(NamedTuple):
    """A node used to mask out unspecified parts of a tree.

    This node is ignored when mapping functions across the tree e.g. using
    :func:`pytree.tree_map` since it is a container without children. It can
    therefore be used to mask out parts of a tree.
    """
    ...


def masked(inner: base.GradientTransformation, mask: Union[Any, Callable[[base.Params], Any]]) -> base.GradientTransformation:
    """Mask updates so only some are transformed, the rest are passed through.

    For example, it is common to skip weight decay for BatchNorm scale and all
    bias parameters. In many networks, these are the only parameters with only
    one dimension. So, you may create a mask function to mask these out as
    follows::
      mask_fn = lambda p: pytree.tree_map(lambda x: x.ndim != 1, p)
      weight_decay = torchopt.masked(torchopt.add_decayed_weights(0.001), mask_fn)
    You may alternatively create the mask pytree upfront::
      mask = pytree.tree_map(lambda x: x.ndim != 1, params)
      weight_decay = torchopt.masked(torchopt.add_decayed_weights(0.001), mask)
    For the ``inner`` transform, state will only be stored for the parameters that
    have a mask value of ``True``.

    Args:
      inner: Inner transformation to mask.
      mask: a PyTree with same structure as (or a prefix of) the params PyTree, or
        a Callable that returns such a pytree given the params/updates. The leaves
        should be booleans, ``True`` for leaves/subtrees you want to apply the
        transformation to, and ``False`` for those you want to skip. The mask must
        be static for the gradient transformation to be jit-compilable.

    Returns:
      New GradientTransformation wrapping ``inner``.
    """
    ...

AddDecayedWeightsState = base.EmptyState
def add_decayed_weights(weight_decay: float = ..., mask: Optional[Union[Any, Callable[[base.Params], Any]]] = ...) -> base.GradientTransformation:
    """Add parameter scaled by `weight_decay`.

    Args:
        weight_decay: a scalar weight decay rate.
        mask: a tree with same structure as (or a prefix of) the params PyTree,
            or a Callable that returns such a pytree given the params/updates.
            The leaves should be booleans, `True` for leaves/subtrees you want to
            apply the transformation to, and `False` for those you want to skip.

    Returns:
      An (init_fn, update_fn) tuple.
    """
    ...

